#No terminal bash
sudo apt update

#instalar scala
sudo apt-get install scala -y

#Digitar para testar versão, abrir o bin e testa-lo.
scala -version
scala
println("Testing Scala")

#Baixe versão do spark pelo site da Apache, descompacte e salve no diretorio /opt/spark

#Configure a variavel de ambiente no terminal 
nano ~/.bashrc

#adicione e salve:
export SPARK_HOME=/opt/spark
export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin

#Digite no terminal: 
source ~/.bashrc; spark-shell; :help;
start-master.sh

#Valide as portas utilizadas pelo Spark, normalmente os abaixo, senão localizar, tire o comando grep e veja a lista geral.
ss -tpln | grep 8081
ss -tpln | grep 8082
ss -tpln | grep 7077


#Instalando pyspark, no terminal digite
sudo pip install pyspark


#passe o owner da pasta para o usuário do zookeeper
sudo chown -R zookeeper:zookeeper /opt/spark

#devemos criar um arquivo para subir o serviço do Spark Master, sendo: 
********Create a Systemd File
sudo nano /etc/systemd/system/spark-master.service

[Unit]
Description=Apache Spark Master
Requires=network.target
After=network.target

[Service]
Type=forking
User=zookeeper
Group=zookeeper
ExecStart=/opt/spark/sbin/start-master.sh
ExecStop=/opt/spark/sbin/stop-master.sh
TimeoutSec=30
Restart=on-failure

[Install]
WantedBy=multi-user.target


#devemos criar outro arquivo para subir o serviço do Spark Worker, sendo: 
sudo nano /etc/systemd/system/spark-worker.service
[Unit]
Description=Apache Spark Worker
After=network.target

[Service]
Type=forking
User=zookeeper
Group=zookeeper
ExecStart=/opt/spark/sbin/start-worker.sh spark://127.0.0.1:7077
ExecStop=/opt/spark/sbin/stop-worker.sh
TimeoutSec=30
Restart=on-failure

[Install]
WantedBy=multi-user.target

#Executar no terminal:
sudo systemctl daemon-reload
sudo systemctl enable spark-master
sudo systemctl enable spark-worker
sudo systemctl start spark-master
sudo systemctl start spark-worker
